{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = wikipedia.page(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def annotate_eos_n_number(string):\n",
    "    string = re.sub(r'([a-zA-Z]{2,}|[0-9]{1,})\\.([ \\n])', r'\\g<1> <eos> ', string)\n",
    "    string = re.sub(r'\\b[0-9\\.]+\\b', '<number>', string)\n",
    "    string = re.sub(r'([^ ]{1,})<number>([^ ]{1,})', '\\g<1> <number> \\g<2>', string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`<>]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r' {2,}', ' ', string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = collections.Counter(sentences)\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches(inputs, batch_size, seq_length):\n",
    "    num_batches = int(inputs.size / (batch_size * seq_length))\n",
    "    if num_batches==0:\n",
    "        assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "    inputs = inputs[:num_batches * batch_size * seq_length]\n",
    "    xdata = inputs\n",
    "    ydata = np.copy(inputs)\n",
    "\n",
    "    ydata[:-1] = xdata[1:]\n",
    "    ydata[-1] = xdata[0]\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), num_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), num_batches, 1)\n",
    "    return num_batches, x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "seq_length = 25\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "grad_clip = 5.\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ny.content\n",
    "text = annotate_eos_n_number(text)\n",
    "text = clean_str(text)\n",
    "words = text.split(' ')\n",
    "vocab, vocab_inv = build_vocab(words)\n",
    "vocab_size = len(vocab_inv)\n",
    "raw_inputs = np.array(list(map(vocab.get, words)))\n",
    "#labels = np.array(list(map(lambda x: 1 if x == '<eos>' else 0, words)))\n",
    "num_batches, x_batches, y_batches = create_batches(raw_inputs, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define LSTM cells\n",
    "cell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell = rnn_cell.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup variables\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "        inputs = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop(prev, _):\n",
    "    prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, last_state = seq2seq.rnn_decoder(inputs, initial_state, cell, \n",
    "                                          loop_function=loop, scope='rnnlm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "loss = seq2seq.sequence_loss_by_example([logits],\n",
    "        [tf.reshape(targets, [-1])],\n",
    "        [tf.ones([batch_size * seq_length])],\n",
    "        vocab_size)\n",
    "\n",
    "cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.run(tf.assign(lr, learning_rate * (decay_rate ** 0)))\n",
    "state = session.run(initial_state)\n",
    "x = x_batches[0]\n",
    "y = y_batches[0]\n",
    "feed = {input_data: x, targets: y, initial_state: state}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/18 (epoch 0), train_loss = 8.380, time/batch = 0.873\n",
      "1/18 (epoch 0), train_loss = 8.330, time/batch = 0.478\n",
      "2/18 (epoch 0), train_loss = 8.136, time/batch = 0.496\n",
      "3/18 (epoch 0), train_loss = 7.477, time/batch = 0.534\n",
      "4/18 (epoch 0), train_loss = 7.066, time/batch = 0.487\n",
      "5/18 (epoch 0), train_loss = 6.623, time/batch = 0.491\n",
      "6/18 (epoch 0), train_loss = 6.540, time/batch = 0.482\n",
      "7/18 (epoch 0), train_loss = 6.194, time/batch = 0.476\n",
      "8/18 (epoch 0), train_loss = 6.211, time/batch = 0.490\n",
      "9/18 (epoch 1), train_loss = 6.334, time/batch = 0.503\n",
      "10/18 (epoch 1), train_loss = 6.063, time/batch = 0.475\n",
      "11/18 (epoch 1), train_loss = 5.893, time/batch = 0.474\n",
      "12/18 (epoch 1), train_loss = 5.896, time/batch = 0.488\n",
      "13/18 (epoch 1), train_loss = 5.866, time/batch = 0.546\n",
      "14/18 (epoch 1), train_loss = 5.852, time/batch = 0.489\n",
      "15/18 (epoch 1), train_loss = 5.826, time/batch = 0.610\n",
      "16/18 (epoch 1), train_loss = 5.686, time/batch = 0.527\n",
      "17/18 (epoch 1), train_loss = 5.880, time/batch = 0.481\n",
      "model saved to ./wikiModel/wiki_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_dir = './wikiModel'\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "        state = sess.run(initial_state)\n",
    "        for b in range(num_batches):\n",
    "            start = time.time()\n",
    "            x = x_batches[b]\n",
    "            y = y_batches[b]\n",
    "            feed = {input_data: x, targets: y, initial_state: state}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            end = time.time()\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * num_batches + b,\n",
    "                        num_epochs * num_batches,\n",
    "                        e, train_loss, end - start))\n",
    "            \n",
    "    checkpoint_path = os.path.join(save_dir, 'wiki_model.ckpt')\n",
    "    saver.save(sess, checkpoint_path, global_step = e * num_batches)\n",
    "    print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_type = 1\n",
    "def predict(sess, words, vocab, prime = \"first all\"):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    if not len(prime) or prime == \" \":\n",
    "        prime  = random.choice(list(vocab.keys()))    \n",
    "    print (prime)\n",
    "    for word in prime.split()[:-1]:\n",
    "        print (word)\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab.get(word,0)\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "    \n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    word = prime.split()[-1]\n",
    "    for n in range(10):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab.get(word,0)\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [prob, state] = sess.run([probs, final_state], feed)\n",
    "        p = prob[0]\n",
    "\n",
    "        if sampling_type == 0:\n",
    "            sample = np.argmax(p)\n",
    "        elif sampling_type == 2:\n",
    "            if word == '\\n':\n",
    "                sample = weighted_pick(p)\n",
    "            else:\n",
    "                sample = np.argmax(p)\n",
    "        else: # sampling_type == 1 default:\n",
    "            sample = weighted_pick(p)\n",
    "\n",
    "        pred = words[sample]\n",
    "        ret += ' ' + pred\n",
    "        word = pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first all\n",
      "first\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 1) for Tensor u'Placeholder:0', which has shape '(50, 25)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-37c60f667723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m        \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m        \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-5d20758d1173>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(sess, words, vocab, prime)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mweighted_pick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    554\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 1) for Tensor u'Placeholder:0', which has shape '(50, 25)'"
     ]
    }
   ],
   "source": [
    " with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        predict(sess, words, vocab)\n",
    "        \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((1, 1))\n",
    "x[0, 0] = vocab.get(\"sunday\",0)\n",
    "feed = {input_data: x, initial_state:state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
