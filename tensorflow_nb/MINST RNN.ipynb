{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.rnn_cell.BasicLSTMCell"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.576499, Training Accuracy= 0.51562\n",
      "Iter 2560, Minibatch Loss= 1.426609, Training Accuracy= 0.54688\n",
      "Iter 3840, Minibatch Loss= 1.016546, Training Accuracy= 0.70312\n",
      "Iter 5120, Minibatch Loss= 0.810375, Training Accuracy= 0.77344\n",
      "Iter 6400, Minibatch Loss= 0.687655, Training Accuracy= 0.75781\n",
      "Iter 7680, Minibatch Loss= 1.008007, Training Accuracy= 0.63281\n",
      "Iter 8960, Minibatch Loss= 0.736357, Training Accuracy= 0.75781\n",
      "Iter 10240, Minibatch Loss= 0.523742, Training Accuracy= 0.85156\n",
      "Iter 11520, Minibatch Loss= 0.380127, Training Accuracy= 0.88281\n",
      "Iter 12800, Minibatch Loss= 0.587432, Training Accuracy= 0.78906\n",
      "Iter 14080, Minibatch Loss= 0.420517, Training Accuracy= 0.90625\n",
      "Iter 15360, Minibatch Loss= 0.273244, Training Accuracy= 0.92188\n",
      "Iter 16640, Minibatch Loss= 0.340321, Training Accuracy= 0.89844\n",
      "Iter 17920, Minibatch Loss= 0.257053, Training Accuracy= 0.92188\n",
      "Iter 19200, Minibatch Loss= 0.176271, Training Accuracy= 0.92969\n",
      "Iter 20480, Minibatch Loss= 0.136715, Training Accuracy= 0.97656\n",
      "Iter 21760, Minibatch Loss= 0.413847, Training Accuracy= 0.84375\n",
      "Iter 23040, Minibatch Loss= 0.131943, Training Accuracy= 0.96094\n",
      "Iter 24320, Minibatch Loss= 0.321178, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 0.334886, Training Accuracy= 0.88281\n",
      "Iter 26880, Minibatch Loss= 0.272219, Training Accuracy= 0.89844\n",
      "Iter 28160, Minibatch Loss= 0.287751, Training Accuracy= 0.89844\n",
      "Iter 29440, Minibatch Loss= 0.255959, Training Accuracy= 0.89844\n",
      "Iter 30720, Minibatch Loss= 0.255962, Training Accuracy= 0.89844\n",
      "Iter 32000, Minibatch Loss= 0.142219, Training Accuracy= 0.96094\n",
      "Iter 33280, Minibatch Loss= 0.195868, Training Accuracy= 0.93750\n",
      "Iter 34560, Minibatch Loss= 0.143212, Training Accuracy= 0.96094\n",
      "Iter 35840, Minibatch Loss= 0.153326, Training Accuracy= 0.95312\n",
      "Iter 37120, Minibatch Loss= 0.272362, Training Accuracy= 0.90625\n",
      "Iter 38400, Minibatch Loss= 0.127364, Training Accuracy= 0.95312\n",
      "Iter 39680, Minibatch Loss= 0.204915, Training Accuracy= 0.95312\n",
      "Iter 40960, Minibatch Loss= 0.284319, Training Accuracy= 0.91406\n",
      "Iter 42240, Minibatch Loss= 0.198682, Training Accuracy= 0.90625\n",
      "Iter 43520, Minibatch Loss= 0.113384, Training Accuracy= 0.95312\n",
      "Iter 44800, Minibatch Loss= 0.164585, Training Accuracy= 0.93750\n",
      "Iter 46080, Minibatch Loss= 0.105289, Training Accuracy= 0.96094\n",
      "Iter 47360, Minibatch Loss= 0.212598, Training Accuracy= 0.96094\n",
      "Iter 48640, Minibatch Loss= 0.219859, Training Accuracy= 0.92969\n",
      "Iter 49920, Minibatch Loss= 0.205119, Training Accuracy= 0.93750\n",
      "Iter 51200, Minibatch Loss= 0.103027, Training Accuracy= 0.96875\n",
      "Iter 52480, Minibatch Loss= 0.111647, Training Accuracy= 0.95312\n",
      "Iter 53760, Minibatch Loss= 0.049511, Training Accuracy= 0.98438\n",
      "Iter 55040, Minibatch Loss= 0.177347, Training Accuracy= 0.93750\n",
      "Iter 56320, Minibatch Loss= 0.141453, Training Accuracy= 0.92188\n",
      "Iter 57600, Minibatch Loss= 0.084036, Training Accuracy= 0.98438\n",
      "Iter 58880, Minibatch Loss= 0.175860, Training Accuracy= 0.94531\n",
      "Iter 60160, Minibatch Loss= 0.084208, Training Accuracy= 0.96094\n",
      "Iter 61440, Minibatch Loss= 0.113536, Training Accuracy= 0.98438\n",
      "Iter 62720, Minibatch Loss= 0.112274, Training Accuracy= 0.96875\n",
      "Iter 64000, Minibatch Loss= 0.155254, Training Accuracy= 0.95312\n",
      "Iter 65280, Minibatch Loss= 0.152524, Training Accuracy= 0.95312\n",
      "Iter 66560, Minibatch Loss= 0.124473, Training Accuracy= 0.94531\n",
      "Iter 67840, Minibatch Loss= 0.144321, Training Accuracy= 0.96094\n",
      "Iter 69120, Minibatch Loss= 0.096231, Training Accuracy= 0.96094\n",
      "Iter 70400, Minibatch Loss= 0.171859, Training Accuracy= 0.93750\n",
      "Iter 71680, Minibatch Loss= 0.079633, Training Accuracy= 0.97656\n",
      "Iter 72960, Minibatch Loss= 0.147712, Training Accuracy= 0.96875\n",
      "Iter 74240, Minibatch Loss= 0.122843, Training Accuracy= 0.93750\n",
      "Iter 75520, Minibatch Loss= 0.133050, Training Accuracy= 0.96094\n",
      "Iter 76800, Minibatch Loss= 0.084739, Training Accuracy= 0.95312\n",
      "Iter 78080, Minibatch Loss= 0.118291, Training Accuracy= 0.97656\n",
      "Iter 79360, Minibatch Loss= 0.091643, Training Accuracy= 0.96875\n",
      "Iter 80640, Minibatch Loss= 0.096870, Training Accuracy= 0.96094\n",
      "Iter 81920, Minibatch Loss= 0.096161, Training Accuracy= 0.97656\n",
      "Iter 83200, Minibatch Loss= 0.053073, Training Accuracy= 0.99219\n",
      "Iter 84480, Minibatch Loss= 0.038554, Training Accuracy= 0.99219\n",
      "Iter 85760, Minibatch Loss= 0.043545, Training Accuracy= 0.99219\n",
      "Iter 87040, Minibatch Loss= 0.118333, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 0.162958, Training Accuracy= 0.94531\n",
      "Iter 89600, Minibatch Loss= 0.123518, Training Accuracy= 0.94531\n",
      "Iter 90880, Minibatch Loss= 0.126584, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 0.060185, Training Accuracy= 0.97656\n",
      "Iter 93440, Minibatch Loss= 0.118684, Training Accuracy= 0.94531\n",
      "Iter 94720, Minibatch Loss= 0.139835, Training Accuracy= 0.97656\n",
      "Iter 96000, Minibatch Loss= 0.139356, Training Accuracy= 0.96094\n",
      "Iter 97280, Minibatch Loss= 0.061479, Training Accuracy= 0.97656\n",
      "Iter 98560, Minibatch Loss= 0.140052, Training Accuracy= 0.97656\n",
      "Iter 99840, Minibatch Loss= 0.061383, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
